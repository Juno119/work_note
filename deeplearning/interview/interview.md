# `deepLearning`面试问题

## 理论方面：        
1.书写BP算法推导过程；        
2.书写softmax损失函数            
3.书写交叉熵损失函数                   
4.书写A-softmax损失函数              
5.书写triplet loss损失函数               
6.书写center loss损失函数                   
7.书写感受野推导公式                 
8.书写网络结构推导公式；             
9.caffe是怎么计算梯度；               
10.网络模型参数估计方法；
11.怎样设计网络；              
12.caffe怎么添加层，过程有那些；           
13.BN 层原理以及公式书写，test时设置false默认（minibatch计算），train时设置true（整个网络计算）；          
14.ResNet为什么可以做到这么深，原理是什么。           

## 项目方面        
13.训练数据集多大，怎么处理和清洗数据，数据怎么准备；       
14.CNN网络的测试准确率多少；          
15.CNN网络层数多大，有哪些组成；            
16.faster rcnn原理和注意事项；                
18.阅读英文文献多么，2017年ICCV和CVPR论文关注点是那些；                
19.模型压缩方法有哪些，你用过那些网络；           
20.是否了解LSTM RNN；            
21.怎么优化网络，做过优化有那些。              

mobile net  
shuffe net  

## 吴恩达课程中回答的问题    
1. 为什么卷积在神经网络中如此受用?    
卷积可时模型中参数数量大幅度减少.原因是参数共享和稀疏链接.   
- 参数共享:对图片一部分有作用的一个特征检测器可能在其他部分也有作用.    
- 稀疏链接:卷积输出结果中的每一个值只依赖于输入图片中少量的像素值(比如3x3个角落值).  
另外,卷积神经网络善于捕捉平移不变性.    
2. 为什么要进行实例化探究?   
通过学习别人现有的成熟网络框架,将其应用到自己的项目中去.   
3. ResNet 的核心思想   
- 将 a[l] 的信息直接传送到 z[l+2] 进行 ReLU 非线性激活之前(线性激活之后)的位置.
- 有利于解决梯度消失和梯度爆炸问题.    
4. ResNet 会表现的这么好?    
首先它不会使网络性能受到影响.即使梯度消失,还是可以使用 short cut 传过来的值计算出梯度进行训练,因此它非常容易学习恒等(identity)函数.   
5. 迁移学习-transfer learning   
根据自己的样本数量的大小来确定冻结的层数,样本数量越大,你需要冻结的层数会越少.如果你有足够的样本量,你可以只使用下载好的权重作为初始化,而不使用随机初始化.  
6. 数据增广   
一般使用一个线程进行数据增广,将增广后得到的数据传递给其他的线程进行训练.数据增广的方式为:   
- mirror/crop/rotation/shearing/local warping   
- color shift   
7. 目标检测问题   
实质上是 classification + localization 问题.    
8. yolo 中将 ground truth box 分配到网格中的思路?   
计算 ground truth box 的中心,按照中心将物体投射到网格中.因此,不管在粗网格还是细网格中,即使横跨多个网格的一个目标也只会被投射到一个网格中.另外,多个物体被投射到一个网格中的概率也就很小了.    
9. IoU intersection of union   
交集/并集的比值.   
另外, IoU 还被用来表示两个 bounding box 之间的交集/并集的比值. 
10. NMS   
非极大值抑制之前会先将概率小于 0.6 的框移除. 然后再去除其他的重叠框.   








## 工作中解决的重要问题
1. VOC 数据集对误检的影响   

2. 样本种类上去之后,如何单独处理 VOC 数据集和正常样本之间的test/train样本划分.  

3. 保证样本分布均匀的意义何在?   

4. 负样本比例较大会产生什么样的影响?    
