# `deepLearning`面试问题

## 理论方面：        
1.书写BP算法推导过程；        
2.书写softmax损失函数            
3.书写交叉熵损失函数                   
4.书写A-softmax损失函数              
5.书写triplet loss损失函数               
6.书写center loss损失函数                   
7.书写感受野推导公式                 
8.书写网络结构推导公式；             
9.caffe是怎么计算梯度；               
10.网络模型参数估计方法；
11.怎样设计网络；              
12.caffe怎么添加层，过程有那些；           
13.BN 层原理以及公式书写，test时设置false默认（minibatch计算），train时设置true（整个网络计算）；          
14.ResNet为什么可以做到这么深，原理是什么。           

## 项目方面        
13.训练数据集多大，怎么处理和清洗数据，数据怎么准备；       
14.CNN网络的测试准确率多少；          
15.CNN网络层数多大，有哪些组成；            
16.faster rcnn原理和注意事项；                
18.阅读英文文献多么，2017年ICCV和CVPR论文关注点是那些；                
19.模型压缩方法有哪些，你用过那些网络；           
20.是否了解LSTM RNN；            
21.怎么优化网络，做过优化有那些。              

mobile net  
shuffe net  

## 吴恩达课程中回答的问题    
1. 为什么卷积在神经网络中如此受用?    
卷积可使模型中参数数量大幅度减少.原因是参数共享和稀疏链接.   
- 参数共享:对图片一部分有作用的一个特征检测器可能在其他部分也有作用.    
- 稀疏链接:卷积输出结果中的每一个值只依赖于输入图片中少量的像素值(比如3x3个角落值).  
另外,卷积神经网络善于捕捉平移不变性.    
2. 为什么要进行实例化探究?   
通过学习别人现有的成熟网络框架,将其应用到自己的项目中去.   
3. ResNet 的核心思想   
- 将 a[l] 的信息直接传送到 z[l+2] 进行 ReLU 非线性激活之前(线性激活之后)的位置.
- 有利于解决梯度消失和梯度爆炸问题.    
4. 为什么 ResNet 会表现的这么好?    
首先它不会使网络性能受到影响.即使梯度消失,还是可以使用 shortcut 传过来的值计算出梯度进行训练,因此它非常容易学习恒等(identity)函数.   
5. 迁移学习-transfer learning   
根据自己的样本数量的大小来确定冻结的层数,样本数量越大,你需要冻结的层数会越少.如果你有足够的样本量,你可以只使用下载好的权重作为初始化,而不使用随机初始化.  
6. 数据增广   
一般使用一个线程进行数据增广,将增广后得到的数据传递给其他的线程进行训练.数据增广的方式为:   
- mirror/crop/rotation/shearing/local warping   
- color shift   
7. 目标检测问题   
实质上是 classification + localization 问题.    
8. yolo 中将 ground truth box 分配到网格中的思路?   
计算 ground truth box 的中心,按照中心将物体投射到网格中.因此,不管在粗网格还是细网格中,即使横跨多个网格的一个目标也只会被投射到一个网格中.另外,多个物体被投射到一个网格中的概率也就很小了.    
9. IoU intersection of union   
交集/并集的比值.   
另外, IoU 还被用来表示两个 bounding box 之间的交集/并集的比值. 
10. NMS   
非极大值抑制之前会先将概率小于 0.6 的框移除. 然后再去除其他的重叠框.   
11. 什么是 SGD ?   
SGD 全称随机梯度下降法, 具体做法是每次从样本中随机抽取 1 个样本用来计算 Loss 函数, 将相应计算出的梯度作为当前这一步梯度下降的依据.   

## 学习中遇到的需要解释的问题   
1. 为什么不用一次函数,而用 ReLU 函数?   
如果用一次函数,那么神经网络不管有多少层,它都是一个线性变换矩阵.得到的结果是线性层面上的.   
2. SGD(随机梯度下降法) 好还是 full-batch 梯度下降好?   
SGD 的一个特点是计算速度快. full-batch 是对所有样本计算一般之后求出梯度后下降一步; 而 SGD 只随机选取 1 个样本, 因此在计算效率上的差距是显而易见的.   
SGD 的另一个特点是随机性. 随机性像噪声一样带来不确定性, 但是在深度学习的应用中是一种优点, 因为深度学习中面临的基本都是非凸优化的问题. 在 SGD 中梯度有一定的概率是朝着"逃出"极小值的方向,从而避免陷入极值. 同时梯度下降的大方向仍然保持正确.   
3. mining-batch 和 SGD.   
严格来说, mini-batch 和 SGD 不是一回事. 不过随着时间的推移, 现在在深度学习中一旦提到 SGD, 其实多数是指 mini-batch.  
4. 克服非凸优化问题的方法.    
加入冲量.   
SGD 方法.   
5. batch-size, iterations, epoches 之间的关系?   
full-batch: 所有的样本数量.    
batch-size: 每次计算梯度时选取的样本个数.   
iterations: 已经计算的梯度(梯度下降)次数.   
epoches: 遍历了所有训练样本数据集的次数.   
(batch-size * iterations) / full-batch = epoches.   
6. 什么样的数据是属于同一分布?   
图片质量, 图片场景, 目标拍摄角度等.
7. 数据均衡和类均衡采样   
数据均衡是不同类别的各自样本数量基本相等, 使用样本增广解决数据不均衡问题.    
类均衡采样是指每次随机选择 mini-batch 个数据中包含各个类别的数据数量是均匀的, 使用随机打乱 shuffle 来解决类均衡采样问题.   
8. 数据增广方式.  
直接在样本基础上做样本增广只是提高模型准确度的方法之一, 但是不是一个很好的方案, 因为这样增加的数据量有限, 并且还要占用额外的硬盘空间.    
最好的办法是训练的时候实时对数据进行增广, 这样就等效于无限多的随机增广.   
9. 分类和回归的区别是什么?   
回归是获取直线(轮廓), 分类是获取属于某一类别 label 的最大概率.  
分类用的是 SoftMaxWithLoss.   
回归用的是　EuclideanLoss, 计算真值和预测值之间的距离. 如果多个标签合在一起看成一个向量的话, 那么 Loss 函数就是这两个向量之间的欧式距离的平方(也可以看作是 L2 norm 范数).    

10. 有哪些目标检测算法?  
滑窗法: 穷举式搜索, 效率低并且需要考虑物体的长宽比.   
Selective Search(选择性搜索): 对图像中可能包含物体的区域进行搜索,进而提升物体检测的效率. 可能存在物体的区域都应该是有着某种相似性的或者连续的区域.   
不管是滑窗法还是 Selective Search , 这种找出可能包含物体的区域的方法, 统称为 Region Proposal.   
有了 Selective Search 高效率的寻找可能包含物体的方框, 那么接下来只要接个 CNN 提取特征, 然后做个分类不就相当于检测了.   
R-CNN: Region-based Convolutional Neural Network.   
RPN: 将 Selective Search 中传统图像算法用神经网络替代, 因为在 Fast R-CNN 中, Selective Search 成了限制计算效率提升的瓶颈.     
11. 有哪些物体检测的评价方法?  
IOU: 有时候未必合理, 视觉上重合度差不多的两个框, 实际应用中很可能因为分辨率不同得到差异很大的值.    
12. 过拟合是什么原因导致的? 解决过拟合的方法有哪些?  
权重太大导致网络有了记忆功能.  
L2 Norm 正则化.   
DropOut 正则化.
13. 为什么只正则化 w, 而没有加上参数 b ?   
因为 w 是一个高维参数向量, 特别是在 high variance 的情况下. 而 b 只是一个实数, 加上它对正则化也没有太多影响. 可以理解为只对仿射变化中的线性变换部分做了正则化处理.    
14. DropOut 中需要注意的地方.   
(1) DropOut 实现过程的最后需要对本层输出(下一层的输入)进行缩放(除以 keep_prop).   
(2) 在 test 阶段是不使用 DropOut 的.   

## 工作中解决的重要问题
1. VOC 数据集对误检的影响   

2. 样本种类上去之后,如何单独处理 VOC 数据集和正常样本之间的test/train样本划分.  

3. 保证样本分布均匀的意义何在?   

4. 负样本比例较大会产生什么样的影响?    


## 深度学习岗位要求   

1. 负责AI平台图像算法包的设计与开发

任职要求：
1.熟悉深度学习的基本原理 DNN,CNN,RNN, 能熟练使用一种以上深度学习keras, tensorflow, theano, caffe， mxnet等
2.熟悉 Java 或者 Python 语言
3.有 Hadoop/Spark 相关项目开发经验者优先

